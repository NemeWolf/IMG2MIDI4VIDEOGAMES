{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f184bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Protwave --> capas CNN dilatadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d40108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MusicVAE inspiration\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Definición de Parámetros ---\n",
    "# Longitud de la secuencia de entrada (ej. 64 pasos de tiempo)\n",
    "input_seq_length = 16\n",
    "# Dimensión de cada elemento en la secuencia (ej. 128 características por paso)\n",
    "features_dim = 128\n",
    "# Dimensión del espacio latente\n",
    "latent_dim = 64\n",
    "\n",
    "# --- 2. Construcción del Codificador Jerárquico ---\n",
    "\n",
    "# Capa de Entrada\n",
    "# Define la forma de los datos de entrada: (batch_size, timesteps, features)\n",
    "inputs = Input(shape=(input_seq_length, features_dim), name=\"entrada_secuencia\")\n",
    "\n",
    "# Pila de LSTMs (tu idea de resumen progresivo)\n",
    "# Capa 1: Procesa la secuencia original.\n",
    "# return_sequences=True es CRUCIAL para pasar la secuencia completa a la siguiente capa.\n",
    "lstm_layer_1 = LSTM(units=128, return_sequences=True, name=\"lstm_resumen_1\")(inputs)\n",
    "\n",
    "# Capa 2: Recibe la secuencia de salidas de la capa 1 y la procesa de nuevo.\n",
    "# Aprende patrones más abstractos a partir de los patrones de la capa 1.\n",
    "lstm_layer_2 = LSTM(units=64, return_sequences=True, name=\"lstm_resumen_2\")(lstm_layer_1)\n",
    "\n",
    "# Capa 3: La última capa de la pila.\n",
    "# return_sequences=False (valor por defecto) para que solo devuelva el *estado oculto final*.\n",
    "# Este vector es el \"resumen final\" de toda la secuencia, procesado jerárquicamente.\n",
    "final_summary_vector = LSTM(units=256, name=\"resumen_final_lstm\")(lstm_layer_2)\n",
    "\n",
    "# Capas Densas para generar el Espacio Latente\n",
    "# Usamos el resumen final para predecir los parámetros de la distribución latente.\n",
    "z_mean = Dense(latent_dim, name=\"z_mean\")(final_summary_vector)\n",
    "z_log_var = Dense(latent_dim, name=\"z_log_var\")(final_summary_vector)\n",
    "\n",
    "# --- 3. Muestreo del Espacio Latente (Sampling) ---\n",
    "# Función para tomar la media y la varianza y generar un punto del espacio latente.\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    # Muestreo usando el \"reparameterization trick\"\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Capa Lambda para aplicar la función de muestreo\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name=\"z_sampling\")([z_mean, z_log_var])\n",
    "\n",
    "# --- 4. Creación del Modelo Codificador Final ---\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name=\"codificador\")\n",
    "\n",
    "# Imprimimos un resumen de la arquitectura\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196cab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling entre capas LSTM para reducir la longitud de la secuencia\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Lambda, AveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Parámetros\n",
    "input_seq_length = 16\n",
    "features_dim = 128\n",
    "latent_dim = 64\n",
    "\n",
    "# Capa de Entrada\n",
    "inputs = Input(shape=(input_seq_length, features_dim))\n",
    "\n",
    "# --- Bloque 1: Resumen de 64 a 32 pasos ---\n",
    "# La LSTM extrae características sin cambiar la longitud\n",
    "lstm_out_1 = LSTM(128, return_sequences=True)(inputs)\n",
    "# La capa de Pooling reduce la longitud de la secuencia a la mitad\n",
    "pooled_out_1 = AveragePooling1D(pool_size=2)(lstm_out_1) # Shape de salida: (None, 32, 128)\n",
    "\n",
    "# --- Bloque 2: Resumen de 32 a 16 pasos ---\n",
    "lstm_out_2 = LSTM(64, return_sequences=True)(pooled_out_1)\n",
    "pooled_out_2 = AveragePooling1D(pool_size=2)(lstm_out_2) # Shape de salida: (None, 16, 64)\n",
    "\n",
    "# --- Capa final y espacio latente ---\n",
    "# La última LSTM solo devuelve el resumen final (un vector)\n",
    "final_summary_vector = LSTM(64)(pooled_out_2) # Shape de salida: (None, 64)\n",
    "\n",
    "# Capas densas para el espacio latente\n",
    "z_mean = Dense(latent_dim, name=\"z_mean\")(final_summary_vector)\n",
    "z_log_var = Dense(latent_dim, name=\"z_log_var\")(final_summary_vector)\n",
    "\n",
    "# ... (sigue la capa de muestreo Lambda, etc.)\n",
    "# ...\n",
    "\n",
    "# Modelo codificador\n",
    "encoder = Model(inputs, [z_mean, z_log_var], name=\"encoder_con_pooling\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c03ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# con capas encoder-decoder anidados\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# --- Función reutilizable para el bloque de resumen ---\n",
    "def create_summarizer_block(input_seq, output_seq_length, latent_dim, name_prefix):\n",
    "    \n",
    "    # Encoder\n",
    "    _, state_h, state_c = LSTM(latent_dim, return_state=True, name=f\"{name_prefix}_encoder_lstm\")(input_seq)\n",
    "    context_vector = [state_h, state_c]\n",
    "\n",
    "    # Decoder\n",
    "    decoder_input_seq = RepeatVector(output_seq_length, name=f\"{name_prefix}_repeat_vector\")(state_h)\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, name=f\"{name_prefix}_decoder_lstm\")\n",
    "\n",
    "    # Conectar el contexto al decodificador\n",
    "    output_seq = decoder_lstm(decoder_input_seq, initial_state=context_vector)\n",
    "    return output_seq\n",
    "\n",
    "# --- 1. Parámetros para el caso de uso de acordes ---\n",
    "input_seq_length = 16   # 16 acordes en la secuencia\n",
    "features_dim = 84      # 128 notas posibles (vector multi-hot por acorde)\n",
    "final_latent_dim = 64   # Dimensión del espacio latente final\n",
    "\n",
    "# --- 2. Construcción del Codificador VAE ---\n",
    "\n",
    "# Entrada: (batch, 16, 84)\n",
    "main_inputs = Input(shape=(input_seq_length, features_dim), name=\"progresion_acordes\")\n",
    "\n",
    "# --- Bloque 1: Resumen de 16 a 8 pasos ---\n",
    "# Reducimos la secuencia de 16 a 8 acordes abstractos\n",
    "summarized_seq_1 = create_summarizer_block(\n",
    "    main_inputs, \n",
    "    output_seq_length=8, \n",
    "    latent_dim=128, # Dimensionalidad interna del bloque\n",
    "    name_prefix=\"resumen_bloque_1\"\n",
    ")\n",
    "\n",
    "# --- Capas Finales para el Espacio Latente ---\n",
    "# Tomamos la secuencia final de 4 pasos y la resumimos en un único vector\n",
    "final_summary_vector = LSTM(128, name=\"resumen_final_lstm\")(summarized_seq_1)\n",
    "\n",
    "# Proyectamos al espacio latente de 64 dimensiones\n",
    "z_mean = Dense(final_latent_dim, name=\"z_mean\")(final_summary_vector)\n",
    "z_log_var = Dense(final_latent_dim, name=\"z_log_var\")(final_summary_vector)\n",
    "\n",
    "# Función de muestreo VAE\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.keras.backend.random_normal(shape=tf.shape(z_mean))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, name=\"z_sampling\")([z_mean, z_log_var])\n",
    "\n",
    "# --- Creación del Modelo Codificador Final ---\n",
    "chord_encoder = Model(main_inputs, [z_mean, z_log_var, z], name=\"Codificador_Acordes_Anidado\")\n",
    "\n",
    "print(\"### Resumen del Codificador para Progresiones de Acordes ###\")\n",
    "chord_encoder.summary(line_length=120)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
